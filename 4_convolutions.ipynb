{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 4\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.954319\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 9.8%\n",
      "Minibatch loss at step 50: 1.474113\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 51.4%\n",
      "Minibatch loss at step 100: 1.912146\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 62.1%\n",
      "Minibatch loss at step 150: 0.972089\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 200: 0.893977\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 70.5%\n",
      "Minibatch loss at step 250: 1.079778\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 300: 0.623774\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 350: 0.199479\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 400: 0.800273\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 450: 0.205752\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 500: 0.672771\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 550: 0.815203\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 600: 0.434088\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 650: 0.191974\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 700: 0.885188\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 750: 0.398870\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 800: 0.901844\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 850: 0.544364\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 900: 0.457446\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 950: 0.132692\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 1000: 0.224671\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.6%\n",
      "Test accuracy: 88.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "keep_prob = .5\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        shape = pool.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.540913\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 50: 2.301010\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 19.6%\n",
      "Minibatch loss at step 100: 1.923324\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy: 53.7%\n",
      "Minibatch loss at step 150: 1.146264\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 69.5%\n",
      "Minibatch loss at step 200: 0.818438\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 70.8%\n",
      "Minibatch loss at step 250: 1.223506\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 300: 0.548018\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 350: 0.356578\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 400: 0.726615\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 450: 0.240769\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 500: 0.526234\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 550: 0.532297\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 600: 0.422164\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 650: 0.234625\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 700: 0.615308\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 750: 0.349071\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 800: 0.897211\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 850: 0.430462\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 900: 0.389992\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 950: 0.102396\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 1000: 0.299109\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 1050: 0.117593\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 1100: 0.274168\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 1150: 0.935739\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 1200: 0.465539\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 1250: 1.062201\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 1300: 0.255588\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 1350: 0.118863\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 1400: 0.173879\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 1450: 0.135015\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 1500: 0.746389\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 1550: 0.291384\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 1600: 1.009781\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 1650: 0.988611\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 1700: 0.806568\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 1750: 0.306979\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 1800: 0.408031\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 1850: 0.125279\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 1900: 0.851583\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 1950: 0.172554\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 2000: 1.094354\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 2050: 0.455832\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 2100: 0.534461\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 2150: 0.515568\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 2200: 0.862640\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 2250: 0.211546\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 2300: 0.179752\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 2350: 0.116943\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 2400: 0.148035\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 2450: 0.436615\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 2500: 0.405125\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 2550: 0.368711\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 2600: 0.521223\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 2650: 0.133357\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 2700: 0.904135\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 2750: 0.385951\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 2800: 0.709492\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 2850: 0.523908\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 2900: 0.665481\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 2950: 0.405066\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 3000: 0.892750\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 3050: 0.628382\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 3100: 0.168723\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.8%\n",
      "Test accuracy: 92.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3101\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1convshape (5, 5, 1, 16)\n",
      "layer2convshape (5, 5, 16, 32)\n",
      "LeNet1shape: [128, 14, 14, 16]\n",
      "LeNet2shape: [128, 7, 7, 32]\n",
      "hidden1shape [1568, 768]\n",
      "after reshape [128, 1568]\n",
      "LeNet1shape: [10000, 14, 14, 16]\n",
      "LeNet2shape: [10000, 7, 7, 32]\n",
      "hidden1shape [1568, 768]\n",
      "after reshape [10000, 1568]\n",
      "LeNet1shape: [10000, 14, 14, 16]\n",
      "LeNet2shape: [10000, 7, 7, 32]\n",
      "hidden1shape [1568, 768]\n",
      "after reshape [10000, 1568]\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "batch_size = 128\n",
    "patch_size = 5\n",
    "patch_size_2 = 4\n",
    "depth = 16\n",
    "\n",
    "num_hidden_1 = 768\n",
    "num_hidden_2 = 256\n",
    "num_hidden_3 = 64\n",
    "\n",
    "decay_rate=.98\n",
    "learnRate=.05\n",
    "keepProb=.6\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    tf_learn_rate = tf.placeholder(tf.float32)\n",
    "    tf_keep_prob  = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    layerconv1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layerconv1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    print(\"layer1convshape\", layerconv1_weights.get_shape())\n",
    "    layerconv2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth*2], stddev=0.1))\n",
    "    layerconv2_biases = tf.Variable(tf.constant(1.0, shape=[depth*2]))\n",
    "    print(\"layer2convshape\", layerconv2_weights.get_shape())\n",
    "    #layerconv3_weights = tf.Variable(tf.truncated_normal(\n",
    "    #  [patch_size_2, patch_size_2, depth, depth], stddev=0.1))\n",
    "    #layerconv3_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    #print(\"layer3convshape\", layerconv3_weights.get_shape())\n",
    "   \n",
    "\n",
    "    layerhidden1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [1568, num_hidden_1], stddev=.05))\n",
    "      #[image_size //patch_size_2  * image_size // patch_size_2 * depth, num_hidden_1], stddev=.05))\n",
    "    layerhidden1_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden_1]))\n",
    "    layerhidden2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden_1, num_hidden_2], stddev=.05))\n",
    "    layerhidden2_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden_2]))\n",
    "    layerhidden3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden_2, num_hidden_3], stddev=.05))\n",
    "    layerhidden3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden_3]))\n",
    "    layerhidden4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden_3, num_labels], stddev=.05))\n",
    "    layerhidden4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "    # Model.\n",
    "    def convNet(data,weights, biases,stride=[1, 1, 1, 1]):\n",
    "        conv = tf.nn.conv2d(data,  weights, stride, padding='SAME')\n",
    "        hidden = tf.nn.relu(conv +  biases)\n",
    "        pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        return pool\n",
    "    \n",
    "    def model(data, dropout=False):\n",
    "        LeNet1=convNet(data,layerconv1_weights, layerconv1_biases)\n",
    "        print(\"LeNet1shape:\", LeNet1.get_shape().as_list())\n",
    "        LeNet2=convNet(LeNet1,layerconv2_weights, layerconv2_biases)\n",
    "        print(\"LeNet2shape:\", LeNet2.get_shape().as_list())\n",
    "        #LeNet3=convNet(LeNet2,layerconv3_weights, layerconv3_biases)\n",
    "        #print(\"LeNet3shape:\", LeNet3.get_shape().as_list())\n",
    "        conv=LeNet2\n",
    "       \n",
    "        shape = conv.get_shape().as_list()\n",
    "        print(\"hidden1shape\",layerhidden1_weights.get_shape().as_list())\n",
    "        reshape = tf.reshape(conv, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        print(\"after reshape\",reshape.get_shape().as_list())\n",
    "        \n",
    "        #fullyconnected 1\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layerhidden1_weights) + layerhidden1_biases)\n",
    "        if dropout==True:\n",
    "            hidden=tf.nn.dropout(hidden,tf_keep_prob )\n",
    "        #fullyconnected 2\n",
    "        hidden = tf.nn.relu(tf.matmul(hidden, layerhidden2_weights) + layerhidden2_biases)\n",
    "        if dropout==True:\n",
    "            hidden=tf.nn.dropout(hidden,tf_keep_prob-(1-tf_keep_prob)/2 )\n",
    "        #fullyconnected 3\n",
    "        hidden = tf.nn.elu(tf.matmul(hidden, layerhidden3_weights) + layerhidden3_biases)\n",
    "        # if dropout==True:\n",
    "        #    hidden=tf.nn.dropout(hidden,tf_keep_prob )\n",
    "            \n",
    "        #fullyconnected 4\n",
    "        return tf.matmul(hidden, layerhidden4_weights) + layerhidden4_biases\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset, dropout=True)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))#+(\n",
    "#                                                        layerconv1_biases*tf.nn.l2_loss(layerconv1_weights))\n",
    "#                                                        layerconv2_biases*tf.nn.l2_loss(layerconv2_weights))\n",
    "#                                                        layerhidden1_biases*tf.nn.l2_loss(layerhidden1_weights)+\n",
    "#                                                        layerhidden2_biases*tf.nn.l2_loss(layerhidden2_weights)+\n",
    "#                                                        layerhidden3_biases*tf.nn.l2_loss(layerhidden3_weights)+\n",
    "#                                                        layerhidden4_biases*tf.nn.l2_loss(layerhidden4_weights))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(tf_learn_rate, global_step,batch_size, decay_rate)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "#    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.746350\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 10.0%\n",
      "--Test accuracy: 10.0%, Runtime: 0.286\n",
      "Minibatch loss at step 50: 2.132012\n",
      "Minibatch accuracy: 17.2%\n",
      "Validation accuracy: 44.3%\n",
      "Minibatch loss at step 100: 1.502257\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy: 65.2%\n",
      "Minibatch loss at step 150: 1.136894\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 200: 1.029634\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 250: 1.238623\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 300: 0.805428\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 350: 0.836615\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 400: 0.581311\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 450: 0.782381\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 500: 0.621608\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 83.2%\n",
      "--Test accuracy: 89.0%, Runtime: 5.503\n",
      "Minibatch loss at step 550: 0.574334\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 600: 0.660362\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 650: 0.767957\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 700: 0.592618\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 750: 0.580776\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 800: 0.647420\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 850: 0.511988\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 900: 0.358052\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 950: 0.497204\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 1000: 0.454435\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.1%\n",
      "--Test accuracy: 90.9%, Runtime: 10.253\n",
      "Minibatch loss at step 1050: 0.537976\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 1100: 0.619554\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 1150: 0.567299\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 1200: 0.525396\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 1250: 0.324837\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 1300: 0.615972\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 1350: 0.760351\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 1400: 0.456224\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 1450: 0.541729\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 1500: 0.504535\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.5%\n",
      "--Test accuracy: 91.9%, Runtime: 14.353\n",
      "Minibatch loss at step 1550: 0.591348\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 1600: 0.457187\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 1650: 0.608427\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 1700: 0.521020\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 1750: 0.306448\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 1800: 0.377061\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 1850: 0.567978\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 1900: 0.492727\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 1950: 0.469131\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 2000: 0.838660\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 87.3%\n",
      "--Test accuracy: 92.6%, Runtime: 18.167\n",
      "Minibatch loss at step 2050: 0.547803\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 2100: 0.605065\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 2150: 0.423309\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 2200: 0.486331\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 2250: 0.543777\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 2300: 0.554270\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 2350: 0.372953\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 2400: 0.306009\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 2450: 0.520349\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 2500: 0.387897\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.8%\n",
      "--Test accuracy: 93.4%, Runtime: 21.939\n",
      "Minibatch loss at step 2550: 0.390707\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 2600: 0.313789\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 2650: 0.381887\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 2700: 0.325841\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 2750: 0.469685\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 2800: 0.362480\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 2850: 0.479822\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 2900: 0.394782\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 2950: 0.547602\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 3000: 0.379624\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.2%\n",
      "--Test accuracy: 93.6%, Runtime: 25.488\n",
      "\n",
      "Final Test accuracy: 93.6%\n",
      "Total runtime:25.707 min\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "testAccuracy=[]\n",
    "\n",
    "startTime0 = time()\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,tf_learn_rate:learnRate, tf_keep_prob:keepProb}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "        if (step % 500 == 0): \n",
    "            timeDelta=round((time()-startTime0)/60, 3)\n",
    "            testAccuracy.append( accuracy(test_prediction.eval(), test_labels))\n",
    "            print('--Test accuracy: %.1f%%, Runtime:' % testAccuracy[-1],timeDelta)\n",
    "    print('\\nFinal Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "timeDelta=round((time()-startTime0)/60, 3)\n",
    "print(\"Total runtime:{} min\".format(timeDelta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0oAAAGJCAYAAAC0OcPeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0740093a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:\n",
      "0 0 10.0\n",
      "1 500 89.32\n",
      "2 1000 90.87\n",
      "3 1500 92.18\n",
      "4 2000 92.75\n",
      "5 2500 93.26\n",
      "6 3000 93.56\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(testAccuracy[4:],label=\"Test Accuracy\")\n",
    "plt.xlabel('Run')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Test Accuracy per step(*500)\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print(\"test accuracy:\")\n",
    "for i in range(len(testAccuracy)):\n",
    "    print(i,i*500,testAccuracy[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking\n",
    "### last run (same as last good run, currently)\n",
    "test accuracy:\n",
    "0 0 10.0\n",
    "1 500 89.32\n",
    "2 1000 90.87\n",
    "3 1500 92.18\n",
    "4 2000 92.75\n",
    "5 2500 93.26\n",
    "6 3000 93.56\n",
    "\n",
    "### last good run : \n",
    "95.5 (same as running params)\n",
    "### best run: \n",
    " 96.8 (dropout, weight intialization, batch size, num_hidden, 100001 run, extra hidden layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#EOF"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
